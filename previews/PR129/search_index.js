var documenterSearchIndex = {"docs":
[{"location":"Entropies/#Entropies","page":"Entropies","title":"Entropies","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"The Entropies.jl package provides a unified interface for entropy estimation, using for example counting-based, nearest neighbor based, kernel density based or wavelet-based approaches. Some of these entropy estimators are also used to calculate mutual information and transfer entropy. Estimators relevant for  mutual information and transfer entropy estimation are also documented here.","category":"page"},{"location":"Entropies/#Data-format","page":"Entropies","title":"Data format","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"Most of the code in this package assumes that your data is represented by the Dataset-type from DelayEmbeddings.jl, where each observation is a D-dimensional data point represented by a static vector. See the DynamicalSystems.jl documentation for more info. Univariate timeseries given as AbstractVector{<:Real} also work with some estimators, but are treated differently based on which method for probability/entropy estimation is applied.","category":"page"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"Dataset","category":"page"},{"location":"Entropies/#DelayEmbeddings.Dataset","page":"Entropies","title":"DelayEmbeddings.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are contained in the field .data of a dataset, as a standard Julia Vector{SVector}.\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others, and when iterated over, it iterates over its contained points.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges).\n\ndata[i] gives the ith datapoint (returns an SVector)\ndata[v1] will return a vector of datapoints\ndata[v1, :] using a Colon as a second index will return a Dataset of these points\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"Entropies/#API","page":"Entropies","title":"API","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"The main API of this package is contained in two functions:","category":"page"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"probabilities which computes probability distributions of given datasets\ngenentropy which uses the output of probabilities, or a set of   pre-computed Probabilities, to calculate entropies.","category":"page"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"These functions dispatch on subtypes of ProbabilitiesEstimator, which are:","category":"page"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"using Entropies, InteractiveUtils\nsubtypes(ProbabilitiesEstimator)","category":"page"},{"location":"Entropies/#Probabilities","page":"Entropies","title":"Probabilities","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"Probabilities\nprobabilities\nprobabilities!\nProbabilitiesEstimator","category":"page"},{"location":"Entropies/#Entropies.Probabilities","page":"Entropies","title":"Entropies.Probabilities","text":"Probabilities(x) â†’ p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"Entropies/#Entropies.probabilities","page":"Entropies","title":"Entropies.probabilities","text":"probabilities(x::Vector_or_Dataset, est::ProbabilitiesEstimator) â†’ p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator and return them as a Probabilities container (Vector-like). The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more.\n\nThe configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Vector_or_Dataset, Îµ::AbstractFloat) â†’ p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length Îµ, and formally we use est = VisitationFrequency(RectangularBinning(Îµ)) as an estimator, see VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes Îµ without memory overflow and with maximum performance. To obtain the bin information along with p, use binhist.\n\nprobabilities(x::Vector_or_Dataset, n::Integer) â†’ p::Probabilities\n\nSame as the above method, but now each dimension of the data is binned into n::Int equal sized bins instead of bins of length Îµ::AbstractFloat.\n\nprobabilities(x::Vector_or_Dataset) â†’ p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, or other processing (mostly useful when x contains categorical or integer data).\n\n\n\n\n\n","category":"function"},{"location":"Entropies/#Entropies.probabilities!","page":"Entropies","title":"Entropies.probabilities!","text":"probabilities!(args...)\n\nIdentical to probabilities(args...), but allows pre-allocation of temporarily used containers.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"Entropies/#Entropies.ProbabilitiesEstimator","page":"Entropies","title":"Entropies.ProbabilitiesEstimator","text":"An abstract type for probabilities estimators.\n\n\n\n\n\n","category":"type"},{"location":"Entropies/#Generalized-entropy","page":"Entropies","title":"Generalized entropy","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"Entropies.genentropy","category":"page"},{"location":"Entropies/#Entropies.genentropy","page":"Entropies","title":"Entropies.genentropy","text":"genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n\nCompute the generalized order-q entropy of some probabilities returned by the probabilities function. Alternatively, compute entropy from pre-computed Probabilities.\n\ngenentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the entropy of the result (and thus est can be a ProbabilitiesEstimator or simply Îµ::Real).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the generalized (RÃ©nyi) entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\n[RÃ©nyi1960]: A. RÃ©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"Entropies/#Fast-histograms","page":"Entropies","title":"Fast histograms","text":"","category":"section"},{"location":"Entropies/","page":"Entropies","title":"Entropies","text":"Entropies.binhist","category":"page"},{"location":"Entropies/#Entropies.binhist","page":"Entropies","title":"Entropies.binhist","text":"binhist(x::AbstractDataset, Îµ::Real) â†’ p, bins\nbinhist(x::AbstractDataset, Îµ::RectangularBinning) â†’ p, bins\n\nHyper-optimized histogram calculation for x with rectangular binning Îµ. Returns the probabilities p of each bin of the histogram as well as the bins. Notice that bins are the starting corners of each bin. If Îµ isa Real, then the actual bin size is Îµ across each dimension. If Îµ isa RectangularBinning, then the bin size for each dimension will depend on the binning scheme.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"function"},{"location":"predictive_asymmetry/#Predictive-asymmetry","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"","category":"section"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"predictive_asymmetry","category":"page"},{"location":"predictive_asymmetry/#CausalityTools.predictive_asymmetry","page":"Predictive asymmetry","title":"CausalityTools.predictive_asymmetry","text":"predictive_asymmetry(source, target, [cond], \n    estimator::TransferEntropyEstimator, Î·s; \n    dð’¯ = 1, dT = 1, dS = 1, Ï„T = -1, Ï„S = -1, \n    [dC = 1, Ï„C = -1,],\n    normalize::Bool = false, f::Real = 1.0)\n\nCompute the predictive asymmetry[Haaga2020] ð”¸(source â†’ target) over prediction lags  Î·s, using the given transfer entropy estimator and embedding parameters dð’¯, dT,  dS, Ï„T, Ï„S.\n\nIf cond is provided, compute ð”¸(source â†’ target |Â cond). Then, dC and Ï„C controls  the embedding dimension and embedding lag for the conditional variable.\n\nNormalization (hypothesis test)\n\nIf normalize == true (the default), then compute the normalized predictive asymmetry ð’œ. \n\nIn this case, for each eta in Î·s, compute ð’œ(Î·) by normalizing ð”¸(Î·) to some fraction f of the  mean transfer entropy over prediction lags -eta  eta (exluding lag 0). \n\nHaaga et al. (2020)[Haaga2020] uses a normalization with f=1.0 as a built-in hypothesis test,  avoiding more computationally costly surrogate testing.\n\nExamples\n\nIt is recommended to use either the rectangular binning-based methods or the symbolic estimators  for the fastest computations. \n\n# Some example time series\nx, y, z = rand(100), rand(100), rand(100)\n\n# Define prediction lags and estimation method\nÎ·s = 1:5\nmethod = VisitationFrequency(RectangularBinning(5))\n\n# ð”¸(x â†’ y) and  ð”¸(x â†’ y |Â z)\nð”¸reg  = predictive_asymmetry(x, y, method, Î·s, normalize = false)\nð”¸cond = predictive_asymmetry(x, y, z, method, Î·s, normalize = false)\n\n# ð’œ(x â†’ y) and ð’œ(x â†’ y |Â z), using different normalization factors\nð’œreg = predictive_asymmetry(x, y, Î·s, method, f = 1.0) # normalize == true by default\nð’œcond = predictive_asymmetry(x, y, z, Î·s, method, f = 1.5) # normalize == true by default\n\nFor the symbolic estimators, make sure that the maximum prediction lag Î· stays  small. This is because the symbolization procedure uses delay embedding vectors  of dimension m if the motif length is m (so the actual maximum prediction lag  used is maximum(Î·s)*m, which may be too large if maximum(Î·s) is too large).\n\n# Some example time series\nx, y, z = rand(100), rand(100), rand(100)\n\n# Define prediction lags and estimation method\nÎ·s = 1:3 # small prediction lags\nestimator = VisitationFrequency(RectangularBinning(4))\n\n# ð’œ(x â†’ y)\npredictive_asymmetry(x, y, estimator, Î·s, normalize = true) \n\n# ð’œ(x â†’ y |Â z)\npredictive_asymmetry(x, y, z, estimator, Î·s, normalize = true) \n\n[Haaga2020]: Haaga, Kristian AgasÃ¸ster, David Diego, Jo Brendryen, and Bjarte Hannisdal. \"A simple test for causality in complex systems.\" arXiv preprint arXiv:2005.01860 (2020).\n\n\n\n\n\n","category":"function"},{"location":"mutualinfo/#Mutual-information","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"The  mutualinfo also uses estimators from the Entropies.jl for its computations.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo","category":"page"},{"location":"mutualinfo/#TransferEntropy.mutualinfo","page":"Mutual information","title":"TransferEntropy.mutualinfo","text":"Mutual information\n\nMutual information I between (potentially collections of) variables X and Y  is defined as \n\nI(X Y) = sum_y in Y sum_x in X p(x y) log left( dfracp(x y)p(x)p(y) right)\n\nHere, we rewrite this expression as the sum of the marginal entropies, and extend the  definition of I to use generalized RÃ©nyi entropies\n\nI^q(X Y) = H^q(X) + H^q(Y) - H^q(X Y)\n\nwhere H^q(cdot) is the generalized Renyi entropy of order q.\n\nGeneral interface\n\nmutualinfo(x, y, est; base = 2, q = 1)\n\nEstimate mutual information between x and y, I^q(x y), using the provided  entropy/probability estimator est and RÃ©nyi entropy of order q (defaults to q = 1,  which is the Shannon entropy), with logarithms to the given base.\n\nBoth x and y can be vectors or (potentially multivariate) Datasets.\n\nBinning based\n\nmutualinfo(x, y, est::VisitationFrequency{RectangularBinning}; base = 2, q = 1)\n\nEstimate I^q(x y) using a visitation frequency estimator. \n\nSee also VisitationFrequency, RectangularBinning.\n\nKernel density based\n\nmutualinfo(x, y, est::NaiveKernel{Union{DirectDistance, TreeDistance}}; base = 2, q = 1)\n\nEstimate I^q(x y) using a naive kernel density estimator. \n\nIt is possible to use both direct evaluation of distances, and a tree-based approach.  Which approach is faster depends on the application. \n\nSee also NaiveKernel, DirectDistance, TreeDistance.\n\nNearest neighbor based\n\nmutualinfo(x, y, est::KozachenkoLeonenko; base = 2)\nmutualinfo(x, y, est::Kraskov; base = 2)\nmutualinfo(x, y, est::Kraskov1; base = 2)\nmutualinfo(x, y, est::Kraskov2; base = 2)\n\nEstimate I^1(x y) using a nearest neighbor based estimator. Choose between naive  estimation using the KozachenkoLeonenko or Kraskov entropy estimators,  or the improved Kraskov1 and Kraskov2 dedicated I estimators. The  latter estimators reduce bias compared to the naive estimators.\n\nNote: only Shannon entropy is possible to use for nearest neighbor estimators, so the  keyword q cannot be provided; it is hardcoded as q = 1. \n\nSee also KozachenkoLeonenko, Kraskov, Kraskov1,  Kraskov2.\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/#[Transfer-entropy](@ref-transferentropy)","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The transferentropy and mutualinfo functions uses estimators from the Entropies.jl to compute transfer entropy and mutual information, respectively.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"transferentropy","category":"page"},{"location":"TransferEntropy/#TransferEntropy.transferentropy","page":"Transfer entropy","title":"TransferEntropy.transferentropy","text":"Transfer entropy\n\nTransfer entropy between two simultaneously measured scalar time series s(n) and t(n),   s(n) =  s_1 s_2 ldots s_N  and t(n) =  t_1 t_2 ldots t_N , is is defined as \n\nTE(s to t) = sum_i p(s_i t_i t_i+eta) log left( dfracp(t_i+eta Â t_i s_i)p(t_i+eta Â t_i) right)\n\nIncluding more than one historical/future value can be done by defining the vector-valued time series mathcalT^(d_mathcal T eta_mathcal T) = t_i^(d_mathcal T eta_mathcal T) _i=1^N,  T^(d_T tau_T) = t_i^(d_T tau_T) _i=1^N,  S^(d_S tau_S) = s_i^(d_T tau_T) _i=1^N,  and  C^(d_C tau_C) = s_i^(d_C tau_C) _i=1^N.\n\nThe N state vectors for each marginal are either \n\nuniform, of the form x_i^(d_X tau_X) = (x_i x_i-tau x_i-2tau ldots x_i-(dX - 1)tau_X),    with equally spaced state vector entries.\nnon-uniform, of the form x_i^(d_X tau_X) = (x_i x_i-tau_1 x_i-tau_2 ldots x_i-tau_dX),   with non-equally spaced state vector entries tau_1 tau_2 ldots tau_dX,   which can freely chosen.\n\nThe  d_T-dimensional, d_S-dimensional and d_C-dimensional state vectors  comprising T, S and C are constructed with embedding lags  tau_T, tau_S, and tau_C, respectively.  The d_mathcal T-dimensional  future states mathcalT^(d_mathcal T eta_mathcal T) are constructed with prediction lag eta_mathcal T (i.e. predictions go from  present/past states to future states spanning a maximum of  d_mathcal T eta_mathcal T time steps ). Note: in the original transfer entropy paper, only the historical states are defined as  potentially higher-dimensional, while the future states are always scalar.\n\nThe non-conditioned and conditioned generalized forms of the transfer entropy are then\n\nTE(s to t) = sum_i p(ST mathcalT) log left( dfracp(mathcalT Â T S)p(mathcalT Â T) right)\n\nTE(s to t Â c) = sum_i p(ST mathcalT C) log left( dfracp(mathcalT Â T S C)p(mathcalT Â T C) right)\n\nEstimation\n\nTransfer entropy is here estimated by rewriting the above expressions as a sum of marginal  entropies, and extending the definitions above to use RÃ©nyi generalized entropies of order  q as\n\nTE^q(s to t) = H^q(mathcal T T) + H^q(T S) - H^q(T) - H^q(mathcal T T S)\n\nTE^q(s to t  c) = H^q(mathcal T T C) + H^q(T S C) - H^q(T C) - H^q(mathcal T T S C)\n\nwhere H^q(cdot) is the generalized Renyi entropy of order q. \n\nGeneral interface\n\ntransferentropy(s, t, [c], est; base = 2, q = 1, \n    Ï„T = -1, Ï„S = -1, Î·ð’¯ = 1, dT = 1, dS = 1, dð’¯ = 1, [Ï„C = -1, dC = 1])\n\nEstimate transfer entropy from source s to target t, TE^q(s to t), using the  provided entropy/probability estimator est and RÃ©nyi entropy of order-q (defaults to q = 1,  which is the Shannon entropy), with logarithms to the given base. Optionally, condition  on c and estimate the conditional transfer entropy TE^q(s to t  c). \n\nParameters for embedding lags Ï„T, Ï„S, Ï„C, the Î·ð’¯ (prediction lag), and  the embedding dimensions dT, dS, dC, dð’¯ have meanings as explained above.  Here, the convention is to use negative lags to indicate embedding delays for past state  vectors (for the T, S and C marginals), and positive lags to indicate embedding  delays for future state vectors (for the mathcal T marginal). \n\nDefault embedding values use scalar time series for each marginal. Hence, the value(s) of  Ï„T, Ï„S or Ï„C affect the estimated TE only if the corresponding dimension(s)  dT, dS or dC are larger than 1.\n\nThe input series s, t, and c must be equal-length real-valued vectors of length N.\n\nBinning based\n\ntransferentropy(s, t, [c], est::VisitationFrequency{RectangularBinning}; base = 2, q = 1, ...)\n\nEstimate TE^q(s to t) or TE^q(s to t  c) using visitation frequencies over a rectangular binning.\n\ntransferentropy(s, t, [c], est::TransferOperator{RectangularBinning}; base = 2, q = 1, ...)\n\nEstimate TE^q(s to t) or TE^q(s to t  c) using an approximation to the transfer operator over rectangular  binning.\n\nSee also: VisitationFrequency, RectangularBinning.\n\nNearest neighbor based\n\ntransferentropy(s, t, [c], est::Kraskov; base = 2, ...)\ntransferentropy(s, t, [c], est::KozachenkoLeonenko; base = 2, ...)\n\nEstimate TE^1(s to t) or TE^1(s to t  c) using naive nearest neighbor estimators.\n\nNote: only Shannon entropy is possible to use for nearest neighbor estimators, so the  keyword q cannot be provided; it is hardcoded as q = 1. \n\nSee also Kraskov, KozachenckoLeonenko.\n\nKernel density based\n\ntransferentropy(s, t, [c], est::NaiveKernel{Union{TreeDistance, DirectDistance}}; \n    base = 2, q = 1,  ...)\n\nEstimate TE^q(s to t) or TE^q(s to t  c) using naive kernel density estimation of  probabilities.\n\nSee also NaiveKernel, TreeDistance, DirectDistance.\n\nInstantenous Hilbert amplitudes/phases\n\ntransferentropy(s, t, [c], est::Hilbert; base = 2, q = 1,  ...)\n\nEstimate TE^q(s to t) or TE^q(s to t  c) by first applying the Hilbert transform  to s, t (c) and then estimating transfer entropy.\n\nSee also Hilbert, Amplitude, Phase.\n\nSymbolic/permutation\n\ntransferentropy(s, t, [c], est::SymbolicPermutation; \n    base = 2, q = 1, m::Int = 3, Ï„::Int = 1, ...)\ntransferentropy!(symb_s, symb_t, s, t, [c], est::SymbolicPermutation; \n    base = 2, q = 1, m::Int = 3, Ï„::Int = 1, ...)\n\nEstimate TE^q(s to t) or TE^q(s to t  c) using permutation entropy. This is done  by first symbolizing the input series s and t (and c; all of length N) using motifs of  size m and a time delay of Ï„. The series of motifs are encoded as integer symbol time  series preserving the permutation information. These symbol time series are embedded as  usual, and transfer entropy is computed from marginal entropies of that generalized embedding.\n\nOptionally, provide pre-allocated (integer) symbol vectors symb_s and symb_t (and symb_c), where length(symb_s) == length(symb_t) == length(symb_c) == N - (est.m-1)*est.Ï„. This is useful for saving  memory allocations for repeated computations.\n\nSee also SymbolicPermutation.\n\nExamples\n\nDefault estimation (scalar marginals): \n\n# Symbolic estimator, motifs of length 4, uniform delay vectors with lag 1\nest = SymbolicPermutation(m = 4, Ï„ = 1) \n\nx, y = rand(100), rand(100)\ntransferentropy(x, y, est)\n\nIncreasing the dimensionality of the T marginal (present/past states of the target  variable):\n\n# Binning-based estimator\nest = VisitationFrequency(RectangularBinning(4)) \nx, y = rand(100), rand(100)\n\n# Uniform delay vectors when `Ï„T` is an integer (see explanation above)\n# Here t_{i}^{(dT, Ï„T)} = (t_i, t_{i+Ï„}, t_{i+2Ï„}, \\ldots t_{i+(dT-1)Ï„})\n# = (t_i, t_{i-2}, t_{i-4}, \\ldots t_{i-6Ï„})\ntransferentropy(x, y, est, dT = 4, Ï„T = -2)\n\n# Non-uniform delay vectors when `Ï„T` is a vector of integers\n# Here t_{i}^{(dT, Ï„T)} = (t_i, t_{i+Ï„_{1}}, t_{i+Ï„_{2}}, \\ldots t_{i+Ï„_{dT}})\n# = (t_i, t_{i-7}, t_{i-25})\ntransferentropy(x, y, est, dT = 3, Ï„T = [0, -7, -25])\n\nLogarithm bases and the order of the RÃ©nyi entropy can also be tuned:\n\nx, y = rand(100), rand(100)\nest = NaiveKernel(0.3)\ntransferentropy(x, y, est, base = MathConstants.e, q = 2) # TE in nats, order-2 RÃ©nyi entropy\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/#estimators","page":"Transfer entropy","title":"Estimators","text":"","category":"section"},{"location":"TransferEntropy/#Binning-based","page":"Transfer entropy","title":"Binning based","text":"","category":"section"},{"location":"TransferEntropy/#Visitation-frequency","page":"Transfer entropy","title":"Visitation frequency","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"VisitationFrequency\nRectangularBinning","category":"page"},{"location":"TransferEntropy/#Entropies.VisitationFrequency","page":"Transfer entropy","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r.\n\nExample\n\n# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\nb = RectangularBinning(5)\n\n# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n# over the boxes of the binning, constructed as describedon the previous line.\nest = VisitationFrequency(b)\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.RectangularBinning","page":"Transfer entropy","title":"Entropies.RectangularBinning","text":"RectangularBinning(Ïµ) <: RectangularBinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme Ïµ.  Binning instructions are deduced from the type of Ïµ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning  is applied, as follows:\n\nÏµ::Int divides each coordinate axis into Ïµ equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.\nÏµ::Float64 divides each coordinate axis into intervals of fixed size Ïµ, starting   from the axis minima until the data is completely covered by boxes.\nÏµ::Vector{Int} divides the i-th coordinate axis into Ïµ[i] equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.\nÏµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size Ïµ[i], starting   from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\nÏµ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals   along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides   each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into 10 equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into 10 equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. \n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid  that is fixed over the intervals [xâ‚, xâ‚‚] for the first dimension, over [yâ‚, yâ‚‚] for the second dimension, and over [zâ‚, zâ‚‚] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded). \n\nThe following binning specification produces the desired (hyper-)rectangular boxes. \n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nxâ‚, xâ‚‚ = 0.5, 1 # not completely covering the data, which are on [0, 1]\nyâ‚, yâ‚‚ = -2, 1.5 # covering the data, which are on [0, 1]\nzâ‚, zâ‚‚ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nÏµ = [(xâ‚, xâ‚‚), (yâ‚, yâ‚‚), (zâ‚, zâ‚‚)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(Ïµ)\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Transfer-operator","page":"Transfer entropy","title":"Transfer operator","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"TransferOperator\ninvariantmeasure\nInvariantMeasure\ntransfermatrix","category":"page"},{"location":"TransferEntropy/#Entropies.TransferOperator","page":"Transfer entropy","title":"Entropies.TransferOperator","text":"TransferOperator(Ïµ::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by  the binning scheme Ïµ, then approxmating the transfer (Perron-Frobenius) operator  over the bins, the taking the invariant measure associated with that transfer operator  as the bin probabilities. Assumes that the input data are sequential.\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition  probabilities between the states defined by the partition elements, where N is the  number of boxes in the partition that is visited by the orbit/points. \n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over  which the transfer operator is approximated,  C_k=1^N  are the N different  partition elements (as dictated by Ïµ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n Â phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points  that are initially in box C_i end up in box C_j when the points in C_i are  projected one step forward in time. Thus, the row P_ik^N where  k in 1 2 ldots N  gives the probability  of jumping from the state defined by box C_i to any of the other N states. It  follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right  stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where  mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row  eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution  mathbfrho^N approximates the invariant density of the system subject to the  partition Ïµ, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using  invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N.  The resulting length-N distribution is then applied to P^N again. This process  repeats until the difference between the distributions over consecutive iterations is  below some threshold. \n\nProbability and entropy estimation\n\nprobabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning}) estimates    probabilities for the bins defined by the provided binning (est.Ïµ)\ngenentropy(x::AbstractDataset, est::TransferOperator{RectangularBinning}) does the same,    but computes generalized entropy using the probabilities.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.invariantmeasure","page":"Transfer entropy","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, Ïµ::RectangularBinning) â†’ iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into  rectangular boxes dictated by the binning scheme Ïµ, then approximate the transfer  (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator,  compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins \ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) â†’ (Ï::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins.  The element Ï[i] is the probability of visitation to the box bins[i]. Analogous to  binhist. \n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain  probabilities? In fact, the naive histogram approach and the  transfer operator approach are equivalent in the limit of long enough time series  (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that  orbits visit a certain region of the state space. The transfer operator encodes that  information too, but comes with the added benefit of knowing the transition  probabilities between states (see transfermatrix). \n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/#Entropies.InvariantMeasure","page":"Transfer entropy","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, Ï)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant  measure Ï, as well as the transfer operator to from which it is computed (including  bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.transfermatrix","page":"Transfer entropy","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) â†’ (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds  to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the  probability of jumping from the state defined by bins[i] to the state defined by  bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/#Kernel-density-based","page":"Transfer entropy","title":"Kernel density based","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"NaiveKernel\nTreeDistance\nDirectDistance","category":"page"},{"location":"TransferEntropy/#Entropies.NaiveKernel","page":"Transfer entropy","title":"Entropies.NaiveKernel","text":"NaiveKernel(Ïµ::Real, method::KernelEstimationMethod = TreeDistance()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by  counting how many other points occupy the space spanned by  a hypersphere of radius Ïµ around mathbfx, according to:\n\nP_i( mathbfx epsilon) approx dfrac1N sum_s neq i  Kleft( dfracmathbfx_i - mathbfx_s epsilon right)\n\nwhere K(z) = 1 if z  1 and zero otherwise. Probabilities are then normalized.\n\nMethods\n\nTree-based evaluation of distances using TreeDistance. Faster, but more   memory allocation.\nDirect evaluation of distances using DirectDistance. Slower, but less    memory allocation. Also works for complex numbers.\n\nEstimation\n\nProbabilities or entropies can be estimated from Datasets.\n\nprobabilities(x::AbstractDataset, est::NaiveKernel). Associates a probability p to    each point in x.\ngenentropy(x::AbstractDataset, est::NaiveKernel).  Associate probability p to each    point in x, then compute the generalized entropy from those probabilities.\n\nExamples\n\nusing Entropy, DelayEmbeddings\npts = Dataset([rand(5) for i = 1:10000]);\nÏµ = 0.2\nest_direct = NaiveKernel(Ïµ, DirectDistance())\nest_tree = NaiveKernel(Ïµ, TreeDistance())\n\np_direct = probabilities(pts, est_direct)\np_tree = probabilities(pts, est_tree)\n\n# Check that both methods give the same probabilities\nall(p_direct .== p_tree)\n\nSee also: DirectDistance, TreeDistance.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.TreeDistance","page":"Transfer entropy","title":"Entropies.TreeDistance","text":"TreeDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated using a tree-based approach with the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.DirectDistance","page":"Transfer entropy","title":"Entropies.DirectDistance","text":"DirectDistance(metric::M = Euclidean()) <: KernelEstimationMethod\n\nPairwise distances are evaluated directly using the provided metric.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Nearest-neighbor-based","page":"Transfer entropy","title":"Nearest neighbor based","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"KozachenkoLeonenko\nKraskov\nKraskov1\nKraskov2","category":"page"},{"location":"TransferEntropy/#Entropies.KozachenkoLeonenko","page":"Transfer entropy","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko(; w::Int = 0) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on nearest neighbors. This implementation is based on Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in CharzyÅ„ska and Gambin (2016)[CharzyÅ„ska2016].\n\nw is the Theiler window (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities cannot be obtained directly.\n\n[CharzyÅ„ska2016]: CharzyÅ„ska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Entropies.Kraskov","page":"Transfer entropy","title":"Entropies.Kraskov","text":"k-th nearest neighbour(kNN) based\n\nKraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n\nEntropy estimator based on k-th nearest neighbor searches[Kraskov2004]. w is the number of nearest neighbors to exclude when searching for neighbours  (defaults to 0, meaning that only the point itself is excluded).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities cannot be obtained directly.\n\n[Kraskov2004]: Kraskov, A., StÃ¶gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#TransferEntropy.Kraskov1","page":"Transfer entropy","title":"TransferEntropy.Kraskov1","text":"Kraskov1(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(1) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#TransferEntropy.Kraskov2","page":"Transfer entropy","title":"TransferEntropy.Kraskov2","text":"Kraskov2(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(2)(x y) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Permutation-based","page":"Transfer entropy","title":"Permutation based","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"SymbolicPermutation","category":"page"},{"location":"TransferEntropy/#Entropies.SymbolicPermutation","page":"Transfer entropy","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; Ï„ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicWeightedPermutation(; Ï„ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicAmplitudeAwarePermutation(; Ï„ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n\nSymbolic, permutation-based probabilities/entropy estimators.\n\nUses embedding dimension m = 3 with embedding lag tau = 1 by default. The minimum dimension m is 2 (there are no sorting permutations of single-element state vectors).\n\nRepeated values during symbolization\n\nIn the original implementation of permutation entropy [BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword lt accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (lt = Entropies.isless_rand). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n\nTo get the behaviour described in Bandt and Pompe (2002), use lt = Base.isless).\n\nProperties of original signal preserved\n\nSymbolicPermutation: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt & Pompe et al. (2002)[BandtPompe2002] and   Berger et al. (2019) [Berger2019].\nSymbolicWeightedPermutation: Like SymbolicPermutation, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)[Fadlallah2013].\nSymbolicAmplitudeAwarePermutation: Like SymbolicPermutation, but also encodes   amplitude information by considering a weighted combination of absolute amplitudes   of state vectors, and relative differences between elements of state vectors. See   description below for explanation of the weighting parameter A. This implementation   is based on Azami & Escudero (2016) [Azami2016].\n\nProbability estimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicProbabilityEstimator). Constructs state vectors   from x using embedding lag Ï„ and embedding dimension m, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; Î±=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est),   then computer the order-Î± generalized entropy to the given base.\n\nSpeeding up repeated computations\n\nA pre-allocated integer symbol array s can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n\nNote: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that length(x) == length(s) if x is a Dataset, or length(s) == length(x) - (m-1)Ï„ if x is a univariate signal that is to be embedded first.\n\nUse the following signatures (only works for SymbolicPermutation).\n\nprobabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) â†’ ps::Probabilities\nprobabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) â†’ ps::Probabilities\n\nMultivariate datasets\n\nAlthough not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension â‰¥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the L existing state vectors mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator). Compute ordinal patterns of the   state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\ngenentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator). Computes probabilities from   symbol frequencies using probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator),   then computes the order-Î± generalized (permutation) entropy to the given base.\n\nCaveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nAll symbolic estimators use the same underlying approach to estimating probabilities.\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.\n\nEach unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nSymbolicPermutation\n\nThe probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [Fadlallah2013]),\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) sum_k=1^N mathbf1_uS(u) in Pi left(mathbfx_k^m tau right) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) N\n\nwhere the function mathbf1_A(u) is the indicator function of a set A. That     is, mathbf1_A(u) = 1 if u in A, and mathbf1_A(u) = 0 otherwise.\n\nSymbolicAmplitudeAwarePermutation\n\nAmplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N mathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1 sum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSymbolicWeightedPermutation\n\nWeighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0). Weights are dictated by the variance of the state vectors.\n\nLet the aritmetic mean of state vector mathbfx_i be denoted by\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nWeights are then computed as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nNote: in equation 7, section III, of the original paper, the authors write\n\nw_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2\n\nBut given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\nEntropy computation\n\nThe generalized order-Î± Renyi entropy[RÃ©nyi1960] can be computed over the probability distribution of symbols as H(m tau alpha) = dfracalpha1-alpha log left( sum_j=1^R p_j^alpha right). Permutation entropy, as described in Bandt and Pompe (2002), is just the limiting case as Î± to1, that is H(m tau) = - sum_j^R p(pi_j^m tau) ln p(pi_j^m tau).\n\nNote: Do not confuse the order of the generalized entropy (Î±) with the order m of the permutation entropy (which controls the symbol size). Permutation entropy is usually estimated with Î± = 1, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[RÃ©nyi1960]: A. RÃ©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#Hilbert","page":"Transfer entropy","title":"Hilbert","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Hilbert\nAmplitude\nPhase","category":"page"},{"location":"TransferEntropy/#TransferEntropy.Hilbert","page":"Transfer entropy","title":"TransferEntropy.Hilbert","text":"Hilbert(est; \n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are  obtained by first applying the Hilbert transform to each signal, then extracting the  phases/amplitudes of the resulting complex numbers[Palus2014]. Original time series are  thus transformed to instantaneous phase/amplitude time series. Transfer  entropy is then estimated using the provided est on those phases/amplitudes (use e.g.  VisitationFrequency, or SymbolicPermutation).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information)  following the phase/amplitude extraction step is not given in Palus (2014). Here,  after instantaneous phases/amplitudes have been obtained, these are treated as regular  time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n[Palus2014]: PaluÅ¡, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.\n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#TransferEntropy.Amplitude","page":"Transfer entropy","title":"TransferEntropy.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"TransferEntropy/#TransferEntropy.Phase","page":"Transfer entropy","title":"TransferEntropy.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"example_systems/#Example-systems","page":"Example systems","title":"Example systems","text":"","category":"section"},{"location":"example_systems/#continuous_systems","page":"Example systems","title":"Continuous coupled dynamical systems","text":"","category":"section"},{"location":"example_systems/#Mediated-link","page":"Example systems","title":"Mediated link","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"mediated_link(;uâ‚€ = rand(9), Ï‰x = 1, Ï‰y = 1.015, Ï‰z = 0.985,\n    k = 0.15, l = 0.2, m = 10.0, c = 0.06)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.mediated_link-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.mediated_link","text":"mediated_link(;uâ‚€ = rand(9), Ï‰x = 1, Ï‰y = 1.015, Ï‰z = 0.985,\n    k = 0.15, l = 0.2, m = 10.0, \n    c = 0.06) -> ContinuousDynamicalSystem\n\nInitialise a three-subsystem dynamical system where X and Y are driven by Z. At the default value of the coupling constant c = 0.06, the responses X and Y are already synchronized to the driver Z.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndx_1 = -omega_x x_2 - x_3 + c*(z_1 - x_1) \ndx_2 = omega_x x_1 + k*x_2  \ndx_3 = l + x_3(x_1 - m)  \ndy_1 = -omega_y y_2 - y_3 + c*(z_1 - y_1)  \ndy_2 = omega_y y_1 + k*y_2  \ndy_3 = l + y_3(y_1 - m)  \ndz_1 = -omega_z z_2 - z_3  \ndz_2 = omega_z z_1 + k*z_2  \ndz_3 = l + z_3(z_1 - m)\nendaligned\n\nReferences\n\nKrakovskÃ¡, Anna, et al. \"Comparison of six methods for the detection of   causality in a bivariate time series.\" Physical Review E 97.4 (2018): 042207\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-Lorenz-systems","page":"Example systems","title":"Two bidirectionally coupled 3D Lorenz systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"lorenz_lorenz_bidir(; u0 = rand(6),\n        c_xy = 0.2, c_yx = 0.2,\n        aâ‚ = 10, aâ‚‚ = 28, aâ‚ƒ = 8/3,\n        bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 9/3)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_bidir-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_bidir","text":"lorenz_lorenz_bidir(; u0 = rand(6), \n    c_xy = 0.2, c_yx = 0.2, \n    aâ‚ = 10, aâ‚‚ = 28, aâ‚ƒ = 8/3, \n    bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 9/3) -> ContinuousDynamicalSystem\n\nInitialise a bidirectionally coupled Lorenz-Lorenz system, where each  subsystem is a 3D Lorenz system [1]. Default values for the parameters  aâ‚, aâ‚‚, aâ‚ƒ, bâ‚, bâ‚‚, bâ‚ƒ are as in [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -a_1 (x_1 - x_2) + c_yx(y_1 - x_1) \ndotx_2 = -x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) \ndoty_2 = -y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3\nendaligned\n\nReferences\n\nAmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-Lorenz-systems-forced-by-another-3D-Lorenz-system","page":"Example systems","title":"Two bidirectionally coupled 3D Lorenz systems forced by another 3D Lorenz system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"lorenz_lorenz_lorenz_bidir_forced(; u0 = rand(9),\n        c_xy = 0.1, c_yx = 0.1,\n        c_zx = 0.05, c_zy = 0.05,\n        aâ‚ = 10, aâ‚‚ = 28, aâ‚ƒ = 8/3,\n        bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 8/3,\n        câ‚ = 10, câ‚‚ = 28, câ‚ƒ = 8/3)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_bidir_forced-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_bidir_forced","text":"lorenz_lorenz_lorenz_bidir_forced(; u0 = rand(9), \n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05, \n    aâ‚ = 10, aâ‚‚ = 28, aâ‚ƒ = 8/3,\n    bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 8/3,\n    câ‚ = 10, câ‚‚ = 28, câ‚ƒ = 8/3)\n\nInitialise a system consisting of two bidirectionally coupled 3D Lorenz  systems forced by an external 3D Lorenz system, giving a 9D system.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = - a_1 (x_1 - x_2) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = - x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) + c_zy(z_1 - y_1) \ndoty_2 = - y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3 \ndotz_1 = - c_1 (z_1 - z_2) \ndotz_2 = - z_1 z_3 + c_2 z_1 - z_2 \ndotz_3 = z_1 z_2 - c_3 z_3 \nendaligned\n\nReferences\n\nAmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Three-transitively-connected-3D-Lorenz-systems","page":"Example systems","title":"Three transitively connected 3D Lorenz systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"lorenz_lorenz_lorenz_transitive(;uáµ¢=rand(9),\n        Ïƒâ‚ = 10.0, Ïƒâ‚‚ = 10.0, Ïƒâ‚ƒ = 10.0,\n        Ïâ‚ = 28.0, Ïâ‚‚ = 28.0, Ïâ‚ƒ = 28.0,\n        Î²â‚ = 8/3,  Î²â‚‚ = 8/3,  Î²â‚ƒ = 8.3,\n        câ‚â‚‚ = 1.0, câ‚‚â‚ƒ = 1.0)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_transitive-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_transitive","text":"lorenz_lorenz_lorenz_transitive(;uáµ¢=rand(9),\n            Ïƒâ‚ = 10.0, Ïƒâ‚‚ = 10.0, Ïƒâ‚ƒ = 10.0,\n            Ïâ‚ = 28.0, Ïâ‚‚ = 28.0, Ïâ‚ƒ = 28.0,\n            Î²â‚ = 8/3,  Î²â‚‚ = 8/3,  Î²â‚ƒ = 8.3,\n            câ‚â‚‚ = 1.0, câ‚‚â‚ƒ = 1.0) -> ContinuousDynamicalSystem\n\nInitalise a dynamical system consisting of three coupled Lorenz attractors with a transitive causality chain where Xâ‚ â†’ Xâ‚‚ and Xâ‚‚ â†’ Xâ‚ƒ. In total, the three 3D-subsystems create a 9-dimensional dynamical system.\n\nThe strength of the forcing Xâ‚ â†’ Xâ‚‚ is controlled by the parameter câ‚, and the forcing from Xâ‚‚ â†’ Xâ‚ƒ by câ‚‚. The remaining parameters are the usual parameters for the Lorenz system, where the subscript i refers to the subsystem Xáµ¢. \n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = sigma_1(y_1 - x_1) \ndoty_1 = rho_1 x_1 - y_1 - x_1 z_1 \ndotz_1 = x_1 y_1 - beta_1 z_1 \ndotx_2 =  sigma_2 (y_2 - x_2) + c_12(x_1 - x_2) \ndoty_2 = rho_2 x_2 - y_2 - x_2 z_2 \ndotz_2 = x_2 y_2 - beta_2 z_2 \ndotx_3 = sigma_3 (y_3 - x_3) + c_23 (x_2 - x_3) \ndoty_3 = rho_3 x_3 - y_3 - x_3 z_3 \ndotz_3 = x_3 y_3 - beta_3 z_3\nendaligned\n\nUsage in literature\n\nThis system was studied by Papana et al. (2013) for coupling strengths  c_12 = 0 1 3 5 and c_23 = 0 1 3 5.\n\nReferences\n\nPapana et al., Simulation Study of Direct Causality Measures in Multivariate   Time Series. Entropy 2013, 15(7), 2635-2661; doi:10.3390/e15072635\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-RÃ¶ssler-systems","page":"Example systems","title":"Two bidirectionally coupled 3D RÃ¶ssler systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_rossler_bidir(; u0 = rand(6),\n        Ï‰â‚ = 1.015, Ï‰â‚‚ = 0.985,\n        c_xy = 0.1, c_yx = 0.1,\n        aâ‚ = 0.15, aâ‚‚ = 0.2, aâ‚ƒ = 10,\n        bâ‚ = 0.15, bâ‚‚ = 0.2, bâ‚ƒ = 10)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_rossler_bidir-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_rossler_bidir","text":"rossler_rossler_bidir(; u0 = rand(6), \n    Ï‰â‚ = 1.015, Ï‰â‚‚ = 0.985, \n    c_xy = 0.1, c_yx = 0.1, \n    aâ‚ = 0.15, aâ‚‚ = 0.2, aâ‚ƒ = 10,\n    bâ‚ = 0.15, bâ‚‚ = 0.2, bâ‚ƒ = 10)\n\nInitialise a system of two bidirectionally coupled 3D RÃ¶ssler systems.  This system has been modified from [1] to allow other parameterisations,  but default parameters are as in [1].\n\nThe X and Y subsystems are mostly synchronized for  c_xy > 0.1 or c_yx > 0.1.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -omega_1(x_2 + x_3) + c_yx(y_1 - x_1) \ndotx_2 = omega_1 x_1 + a_1 x_2 \ndotx_3 = a_2 + x_3 (x_1 - a_3) \ndoty_1 = -omega_2 (y_2 + y_3) + c_xy(x_1 - y_1) \ndoty_2 = omega_2 y_1 + b_1 y_2 \ndoty_3 = b_2 + y_3 (y_1 - b_3)\nendaligned\n\nReferences\n\nAmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-RÃ¶ssler-systems-forced-by-another-3D-RÃ¶ssler-system","page":"Example systems","title":"Two bidirectionally coupled 3D RÃ¶ssler systems forced by another 3D RÃ¶ssler system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_rossler_rossler_bidir_forced(; u0 = rand(9),\n        Ï‰â‚ = 1.015, Ï‰â‚‚ = 0.985, Ï‰â‚ƒ = 0.95,\n        c_xy = 0.1, c_yx = 0.1,\n        c_zx = 0.05, c_zy = 0.05,\n        aâ‚ = 0.15, aâ‚‚ = 0.2, aâ‚ƒ = 10,\n        bâ‚ = 0.15, bâ‚‚ = 0.2, bâ‚ƒ = 10,\n        câ‚ = 0.15, câ‚‚ = 0.2, câ‚ƒ = 10)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_rossler_rossler_bidir_forced-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_rossler_rossler_bidir_forced","text":"rossler_rossler_rossler_bidir_forced(; u0 = rand(9), \n    Ï‰â‚ = 1.015, Ï‰â‚‚ = 0.985, Ï‰â‚ƒ = 0.95,\n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05, \n    aâ‚ = 0.15, aâ‚‚ = 0.2, aâ‚ƒ = 10,\n    bâ‚ = 0.15, bâ‚‚ = 0.2, bâ‚ƒ = 10,\n    câ‚ = 0.15, câ‚‚ = 0.2, câ‚ƒ = 10)\n\nEquations of motion for a system consisting of three coupled 3D RÃ¶ssler systems  (X, Y, Z), giving a 9D system [1]. The external system  Z influences both X and Y (controlled by c_zx and c_zy).  Simultaneously, the subsystems  X and Y bidirectionally  influences each other (controlled by c_xy and c_yx).\n\nThe X and Y subsystems are mostly synchronized for c_xy > 0.1 or  c_yx > 0.1.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -omega_1 (x_2 + x_3) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = omega_1 x_1 + a_1 x_2 \ndotx_3 = a_2 + x_3 (x_1 - a_3) \ndoty_1 = -omega_1 (y_2 + y_3) + c_xy(x_1 - y_1) + c_zy(z_1 - y_1) \ndotx_2 = omega_2 y_1 + b_1 y_2 \ndotx_3 = b_2 + x_3 (y_1 - b_3) \ndoty_1 = -omega_2 (z_2  + z_3) \ndotx_2 = omega_2 z_1 + c_1 z_2 \ndotx_3 = c_2 + z_3 (z_1 - c_3)\nendaligned\n\nReferences\n\nAmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302. \n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Unidirectonal-forcing-from-a-3D-RÃ¶ssler-system-to-a-3D-Lorenz-system","page":"Example systems","title":"Unidirectonal forcing from a 3D RÃ¶ssler system to a 3D Lorenz system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_lorenz(;uâ‚€ = rand(6), aâ‚ = -6, aâ‚‚ = 6, aâ‚ƒ = 2.0,\n        bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 8/3, c_xy = 1)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_lorenz-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_lorenz","text":"rossler_lorenz(;uâ‚€ = rand(6), aâ‚ = -6, aâ‚‚ = 6, aâ‚ƒ = 2.0, \n    bâ‚ = 10, bâ‚‚ = 28, bâ‚ƒ = 8/3, c_xy = 1) -> ContinuousDynamicalSystem\n\nInitialise a RÃ¶ssler-Lorenz system consisting of two independent 3D subsystems: one RÃ¶ssler system and one Lorenz system. They are coupled such that the second component (xâ‚‚) of the RÃ¶ssler system unidirectionally forces the second component (yâ‚‚) of the Lorenz system. \n\nThe parameter c_xy controls the coupling strength. The implementation here also  allows for tuning the parameters of each subsystem by introducing the constants  aâ‚, aâ‚‚, aâ‚ƒ, bâ‚, bâ‚‚, bâ‚ƒ. Default values for these parameters are  as in [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndot x_1 = a_1(x_2 + x_3) \ndot x_2 = a_2(x_1 + 02x_2) \ndot x_3 = a_2(02 + x_3(x_1 - a_3)) \ndot y_1 = b_1(y_2 - y_1) \ndot y_2 = y_1(b_2 - y_3) - y_2 +c_xy(x_2)^2 \ndot y_3 = y_1 y_2 - b_3y_3\nendaligned\n\nwith the coupling constant c_xy geq 0.\n\nReferences\n\nKrakovskÃ¡, Anna, et al. \"Comparison of six methods for the detection of causality in a   bivariate time series.\" Physical Review E 97.4 (2018):042207.   https://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#N-scroll-Chua-attractors","page":"Example systems","title":"N-scroll Chua attractors","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"chuacircuit_nscroll_sine(;uâ‚€ = [0.0, 0.0, 0.28695],\n        Î± = 10.814, Î² = 14, Î³ = 0, a = 1.3, b = 0.11, c = 2,\n        Ïƒx = 0.0, Ïƒy = 0.0, Ïƒz = 0.0)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.chuacircuit_nscroll_sine-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.chuacircuit_nscroll_sine","text":"chuacircuit_nscroll_sine(;uâ‚€ = [0.0, 0.0, 0.28695],\n    Î± = 10.814, Î² = 14, Î³ = 0, a = 1.3, b = 0.11, c = 2,\n    Ïƒx = 0.0, Ïƒy = 0.0, Ïƒz = 0.0)\n\nInitialise an adjusted Chua system giving rise to n-scroll attractors [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx = alpha (y - fx) + eta x \ndoty = x - y + z + eta y \ndotz = -beta y - gamma z + eta z \nendaligned\n\nwhere eta x, eta z, and eta z are drawn independently from  normal distributions with zero mean and standard deviations Ïƒx, Ïƒy  and Ïƒz at each iteration.\n\nfx is given by the following conditions: \n\nn::Int = c + 1\n\nif x >= 2*a*c\n    fx = (b*pi/2*a)*(x - 2*a*c)\nelseif -2*a*c < x < 2*a*c\n    d = ifelse(isodd(n), pi, 0)\n    fx = -b*sin((pi*x/2*a) + d)\nelseif x <= -2*a*c\n    fx = (b*pi/2*a)*(x + 2*a*c)\nend\n\nReferences\n\nTang, Wallace KS, et al. \"Generation of n-scroll attractors via   sine function.\" IEEE Transactions on Circuits and Systems I:   Fundamental Theory and Applications 48.11 (2001): 1369-1372.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#discrete_systems","page":"Example systems","title":"Discrete coupled dynamical systems","text":"","category":"section"},{"location":"example_systems/#system_ar1","page":"Example systems","title":"Autoregressive order one 2D system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ar1_unidir(;uáµ¢ = rand(2), aâ‚ = 0.90693, bâ‚ = 0.40693, c_xy = 0.5, Ïƒ = 0.40662)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.ar1_unidir-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.ar1_unidir","text":"ar1_unidir(uáµ¢, aâ‚ = 0.90693, bâ‚ = 0.40693, c_xy = 0.5, \n    Ïƒ = 0.40662) -> DiscreteDynamicalSystem\n\nA bivariate, order one autoregressive model, where x to y [1].\n\nEquations of motion\n\nbeginaligned\nx(t+1) = a_1 x(t) + xi_1 \ny(t+1) = b_1 y(t) - c_xy x + xi_2\nendaligned\n\nwhere xi_1 and xi_2 are drawn from normal distributions  with zero mean and standard deviation Ïƒ at each iteration.\n\nReferences\n\nPaluÅ¡, M., KrakovskÃ¡, A., JakubÃ­k, J., & ChvostekovÃ¡, M. (2018). Causality,  dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of  Nonlinear Science, 28(7), 075307. http://doi.org/10.1063/1.5019944\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#system_nonlinear3d","page":"Example systems","title":"Nonlinear 3D system with nonlinear coupling","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"nonlinear3d","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.nonlinear3d","page":"Example systems","title":"CausalityTools.ExampleSystems.nonlinear3d","text":"nonlinear3d(;uáµ¢ = rand(3), \n    Ïƒâ‚ = 1.0, Ïƒâ‚‚ = 1.0, Ïƒâ‚ƒ = 1.0, \n    aâ‚ = 3.4, aâ‚‚ = 3.4, aâ‚ƒ = 3.4, \n    bâ‚ = 0.4, bâ‚‚ = 0.4, bâ‚ƒ = 0.4, \n    câ‚â‚‚ = 0.5, câ‚‚â‚ƒ = 0.3, câ‚â‚ƒ = 0.5) -> DiscreteDynamicalSystem\n\nA 3d nonlinear system with nonlinear couplings x_1 to x_2,  x_2 to x_3 and x_1 to x_3. Modified from [1]. \n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx_1(t+1) = a_1 x_1 (1-x_1(t))^2  e^-x_2(t)^2 + 04 xi_1(t) \nx_2(t+1) = a_1 x_2 (1-x_2(t))^2  e^-x_2(t)^2 + 04 xi_2(t) + b x_1 x_2 \nx_3(t+1) = a_3 x_3 (1-x_3(t))^2  e^-x_3(t)^2 + 04 xi_3(t) + c x_2(t) + d x_1(t)^2\nendaligned\n\nReferences\n\nGourÃ©vitch, B., Le Bouquin-JeannÃ¨s, R., & Faucon, G. (2006). Linear and nonlinear   causality between signals: methods, examples and neurophysiological   applications. Biological Cybernetics, 95(4), 349â€“369.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#system_logistic2_unidir","page":"Example systems","title":"Unidirectionally coupled 2D logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic2_unidir(;uâ‚€ = rand(2), c_xy = 0.1, râ‚ = 3.78, râ‚‚ = 3.66, Ïƒ = 0.05)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic2_unidir-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic2_unidir","text":"logistic2(;uâ‚€ = rand(2), c_xy = 0.1, Ïƒ = 0.05,\n    râ‚ = 3.78, râ‚‚ = 3.66) -> DiscreteDynamicalSystem\n\nInitialise a system consisting of two coupled logistic maps where X unidirectionally influences Y. By default, the parameters râ‚ and râ‚‚ are set to values yielding chaotic behaviour.\n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx(t+1) = r_1 x(t)(1 - x(t)) \ny(t+1) = r_2 f(xy)(1 - f(xy))\nendaligned\n\nwith\n\nbeginaligned\nf(xy) = dfracy + fracc_xy(x xi )21 + fracc_xy2(1+ sigma )\nendaligned\n\nThe parameter c_xy controls how strong the dynamical forcing is. If Ïƒ > 0, dynamical noise masking the influence of  x on y equivalent to sigma cdot xi is added at each iteration. Here,xi is a draw from a flat distribution on 0 1. Thus, setting Ïƒ = 0.05 is equivalent to add dynamical noise corresponding to a maximum of 5  of the possible range of values of the logistic map.\n\nReferences\n\nDiego, David, Kristian AgasÃ¸ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#system_logistic2_bidir","page":"Example systems","title":"Bidirectionally coupled 2D logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic2_bidir(;uâ‚€ = rand(2), c_xy = 0.1, c_yx = 0.1,\n    râ‚ = 3.78, râ‚‚ = 3.66, Ïƒ_xy = 0.05, Ïƒ_yx = 0.05)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic2_bidir-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic2_bidir","text":"logistic2_bidir(;uâ‚€ = rand(2), c_xy = 0.1, c_yx = 0.1, \n    râ‚ = 3.78, râ‚‚ = 3.66, Ïƒ_xy = 0.05, Ïƒ_yx = 0.05)\n\nA bidirectional logistic model for the chaotic population dynamics of two interacting  species [1].\n\nEquations of motion\n\nThe equations of motion are \n\nbeginalign\nx(t+1) = r_1 f_yx^t(1 - f_yx^t) \ny(t+1) = r_2 f_xy^t(1 - f_xy^t) \nf_xy^t = dfracy(t) + c_xy(x(t) + sigma_xy xi_xy^t )1 + c_xy (1 + sigma_xy )  \nf_yx^t = dfracx(t) + c_yx(y(t) + sigma_yx xi_yx^t )1 + c_yx (1 + sigma_yx )\nendalign\n\nwhere the coupling strength c_xy controls how strongly species x influences species  y, and vice versa for c_yx. To simulate time-varying influence of unobserved  processes, we use the dynamical noise terms xi_xy^t and xi_yx^t, drawn from a  uniform distribution with support on 0 1. If sigma_xy  0, then the influence  of x on y is masked by dynamical noise equivalent to sigma_xy xi_xy^t at  the t-th iteration of the map, and vice versa for sigma_yx.\n\nReferences\n\nDiego, David, Kristian AgasÃ¸ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Forcing-of-two-independent-logistic-maps-from-common-logistic-map-driver","page":"Example systems","title":"Forcing of two independent logistic maps from common logistic map driver","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic3(;uâ‚€ = rand(3), râ‚ = 4, râ‚‚ = 4, râ‚ƒ = 4, Ïƒx = 0.05, Ïƒy = 0.05, Ïƒz = 0.05)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic3-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic3","text":"logistic3(;uâ‚€ = rand(3), r = 4,\n    Ïƒx = 0.05, Ïƒy = 0.05, Ïƒz = 0.05) -> DiscreteDynamicalSystem\n\nInitialise a dynamical system consisting of three coupled logistic map representing the response of two independent dynamical variables to the forcing from a common driver. The dynamical influence goes in the directions Z to X and Z to Y.\n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + Ïƒ_x Î·_x)) mod 1 \ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + Ïƒ_y Î·_y)) mod 1 \nz(t+1) = (z(t)(r - r_3 z(t) + Ïƒ_z Î·_z)) mod 1\nendaligned\n\nDynamical noise may be added to each of the dynamical variables by tuning the parameters Ïƒz, Ïƒx and Ïƒz. Default values for the parameters râ‚, râ‚‚ and râ‚ƒ are set such that the system exhibits chaotic behaviour, with râ‚ = râ‚‚ = râ‚ƒ = 4.\n\nReferences\n\nRunge, Jakob. Causal network reconstruction from time series: From theoretical   assumptions to practical estimation, Chaos 28, 075310 (2018);   doi: 10.1063/1.5025050\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Unidirectional,-transitive-chain-of-logistic-maps","page":"Example systems","title":"Unidirectional, transitive chain of logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic4(;uâ‚€ = rand(4),\n            râ‚ = 3.9, râ‚‚ = 3.6, râ‚ƒ = 3.6, râ‚„ = 3.8,\n            câ‚â‚‚ = 0.4, câ‚‚â‚ƒ = 0.4, câ‚ƒâ‚„ = 0.35)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic4-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic4","text":"logistic4(;uâ‚€ = rand(4), râ‚ = 3.9, râ‚‚ = 3.6, râ‚ƒ = 3.6, râ‚„ = 3.8,\n    câ‚â‚‚ = 0.4, câ‚‚â‚ƒ = 0.4, câ‚ƒâ‚„ = 0.35) -> DiscreteDynamicalSystem\n\nInitialise a system of a transitive chain of four unidirectionally coupled logistic maps, where y_1 to y_2 to y_3 to y_4 [1]. Default  parameters are as in [1].\n\nNote: With the default parameters which are as in [1], for some initial conditions,  this system wanders off to pm infty for some of the variables. Make sure that  you have a good realisation before using the orbit for anything.\n\nEquations of motion\n\nbeginaligned\ny_1(t+1) = y_1(t)(r_1 - r_1 y_1) \ny_2(t+1) = y_2(t)(r_2 - c_12 y_1 - r_2 y_2) \ny_3(t+1) = y_3(t)(r_3 - c_23 y_2 - r_3 y_3) \ny_4(t+1) = y_4(t)(r_4 - c_34 y_3 - r_4 y_4)\nendaligned\n\nReferences\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using  convergent cross mapping.\" Scientific reports 5 (2015): 14750\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#henon2d","page":"Example systems","title":"Two unidirectionally coupled Henon maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"henon2(;uâ‚€ = rand(4), c_xy = 2.0)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.henon2-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.henon2","text":"henon2(;uâ‚€ = rand(4), c_xy = 2.0) -> DiscreteDynamicalSystem\n\nInitialize a 2D Henon system consisting of two identical Henon maps with unidirectional forcing X to Y [1].\n\nEquations of motion\n\nThe equations of motion are \n\nbeginaligned\nx_1(t+1) = 14 - x_1^2(t) + 03x_2(t) \nx_2(t+1) = x_1(t) \ny_1(t+1) = 14 - c_xy x_1(t) y_1(t) + (1-c_xy) y_1^2(t) + 03 y_2(t) \ny_2(t+1) = y_1(t)\nendaligned\n\nReferences\n\nKrakovskÃ¡, A., JakubÃ­k, J., ChvostekovÃ¡, M., Coufal, D., Jajcay, N., & PaluÅ¡, M. (2018).   Comparison of six methods for the detection of causality in a bivariate time series.   Physical Review E, 97(4), 042207.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Strange,-nonchaotic-attractors","page":"Example systems","title":"Strange, nonchaotic attractors","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"anishchenko1(;uâ‚€ = rand(2), Î± =3.277, s=0.1, Ï‰=0.5*(sqrt(5)-1))","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.anishchenko1-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.anishchenko1","text":"anishchenko1(;uâ‚€ = rand(2), Î± =3.277, s=0.1, Ï‰=0.5*(sqrt(5)-1))\n\nInitialise the system defined by eq. 13 in [1], which can give strange,  nonchaotic attractors.\n\nEquations of motion\n\nThe equations of motion are \n\nbeginaligned\ndx = alpha (1-s cos (2 pi phi )) cdot x(1-x) \ndÏ• = (phi + omega ) mod1\nendaligned\n\nReferences\n\nAnishchenko, Vadim S., and Galina I. Strelkova. \"Irregular attractors.\"  Discrete dynamics in Nature and Society 2.1 (1998): 53-72.\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#ikeda2d_bidir","page":"Example systems","title":"Bidirectional Ikeda map","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ikeda","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.ikeda","page":"Example systems","title":"CausalityTools.ExampleSystems.ikeda","text":"ikeda(; uâ‚€ = rand(2), c_xy = 1.0, c_yx = 1.0, a = 0.8, b = 12, c = 0.9,\n    râ‚ = rand(Uniform(0.01, 0.3)), râ‚‚ = rand(Uniform(0.01, 0.3)), Ïƒ = 0.05)\n\nInitialise a discrete two-dimensional Ikeda map system, adapted from [1] by adding a noise term and allowing the influences from x to y (c_xy) and  from y to x (c_yx) to be adjusted.\n\nAs a rule-of-thumb, if parameters a, b, and c are drawn from uniform  distributions on [0.8, 1.5], [10, 14] and [0.1, 0.9].\n\nThe difference equations are\n\nbeginaligned\nx(t+1) = 1 + mu(x cos(theta) - c_yx y sin(theta)) - min(dfracsigma xi_t^(1))(1-x) xi_t^(2) \ny(t+1) = mu(y cos(theta) - c_xy x sin(theta)) - min(dfracsigma zeta_t^(1))(1-y) zeta_t^(2)\nendaligned\n\nReferences\n\nCao, Liangyue, Alistair Mees, and Kevin Judd. \"Modeling and predicting   non-stationary time series.\" International Journal of Bifurcation and   Chaos 7.08 (1997): 1823-1831.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Noise","page":"Example systems","title":"Noise","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_uu","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_uu","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_uu","text":"noise_uu(n::Int, lo = - 1, hi = 1)\n\nGenerate a signal consisting of n steps of uncorrelated uniform noise from  a uniform distribution on [lo, hi].\n\n\n\n\n\n","category":"function"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_ug","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_ug","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_ug","text":"noise_ug(n::Int; Î¼ = 0, Ïƒ = 1)\n\nGenerate a signal consisting of n steps of uncorrelated Gaussian noise from a normal distribution with mean Î¼ and standard deviation Ïƒ.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_brownian","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_brownian","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_brownian","text":"noise_brownian(n::Int; lo = - 1, hi = 1)\nnoise_brownian(d::Distribution, n::Int)\n\nGenerate a signal consisting of n steps of Brownian noise, generated as the zero-mean and unit standard deviation normalised cumulative sum of noise generated from a uniform distribution on [lo, hi]. Optionally, a distribution d from which to sample can be provided.\n\nExamples\n\n# Based on uncorrelated uniform noise\nnoise_brownian(100)\nnoise_brownian(100, lo = -2, hi = 2)\nnoise_brownian(Uniform(-3, 3), 100)\n\n# Based on uncorrelated Gaussian noise\nÎ¼, Ïƒ = 0, 2\nnoise_brownian(Normal(Î¼, Ïƒ), 100)\n\n\n\n\n\n","category":"function"},{"location":"surrogate/#Surrogate-data","page":"Surrogate data","title":"Surrogate data","text":"","category":"section"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"Surrogate time series can be used for hypothesis testing in time series causality analyses. ","category":"page"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"The TimeseriesSurrogates.jl package provides methods for generating surrogate time series. Relevant methods are re-exported here for convenience. For more details, see its package documentation.","category":"page"},{"location":"surrogate/#Generation-of-surrogate-data","page":"Surrogate data","title":"Generation of surrogate data","text":"","category":"section"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"TimeseriesSurrogates.jl exports two main functions. Both of them dispatch on the chosen method, a subtype of Surrogate.","category":"page"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"surrogate\nsurrogenerator","category":"page"},{"location":"surrogate/#TimeseriesSurrogates.surrogate","page":"Surrogate data","title":"TimeseriesSurrogates.surrogate","text":"surrogate(x, method::Surrogate) â†’ s\n\nCreate a single surrogate timeseries s from x based on the given method. If you want to generate more than one surrogates from x, you should use surrogenerator.\n\n\n\n\n\n","category":"function"},{"location":"surrogate/#TimeseriesSurrogates.surrogenerator","page":"Surrogate data","title":"TimeseriesSurrogates.surrogenerator","text":"surrogenerator(x, method::Surrogate) â†’ sg::SurrogateGenerator\n\nInitialize a generator that creates surrogates of x on demand, based on given method. This is efficient, because for most methods some things can be initialized and reused for every surrogate.\n\nTo generate a surrogate, call sg as a function with no arguments, e.g.:\n\nsg = surrogenerator(x, method)\nfor i in 1:1000\n    s = sg()\n    # do stuff with s and or x\n    result[i] = stuff\nend\n\n\n\n\n\n","category":"function"},{"location":"surrogate/#Surrogate-methods","page":"Surrogate data","title":"Surrogate methods","text":"","category":"section"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"RandomShuffle\nBlockShuffle\nCycleShuffle\nCircShift\nRandomFourier\nTFTS\nAAFT\nTAAFT\nIAAFT\nAutoRegressive\nPseudoPeriodic\nWLS\nShuffleDimensions","category":"page"},{"location":"surrogate/#TimeseriesSurrogates.RandomShuffle","page":"Surrogate data","title":"TimeseriesSurrogates.RandomShuffle","text":"RandomShuffle() <: Surrogate\n\nA random constrained surrogate, generated by shifting values around.\n\nRandom shuffle surrogates preserve the mean, variance and amplitude  distribution of the original signal. Properties not preserved are any  temporal information, such as the power spectrum and hence linear  correlations. \n\nThe null hypothesis this method can test for is whether the data  are uncorrelated noise, possibly measured via a nonlinear function. Specifically, random shuffle surrogate can test  the null hypothesis that the original signal is produced by independent and  identically distributed random variables[^Theiler1991, ^Lancaster2018]. \n\nBeware: random shuffle surrogates do not cover the case of correlated noise[Lancaster2018]. \n\n[Theiler1991]: J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1â€“4) (1992) 77â€“94.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.BlockShuffle","page":"Surrogate data","title":"TimeseriesSurrogates.BlockShuffle","text":"BlockShuffle(n::Int) <: Surrogate\n\nA block shuffle surrogate constructed by dividing the time series into n blocks of roughly equal width at random indices (end blocks are wrapped around to the start of the time series).\n\nBlock shuffle surrogates roughly preserve short-range temporal properties in the time series (e.g. correlations at lags less than the block length), but break any long-term dynamical information (e.g. correlations beyond the block length).\n\nHence, these surrogates can be used to test any null hypothesis aimed at comparing short-range dynamical properties versus long-range dynamical properties of the signal.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.CycleShuffle","page":"Surrogate data","title":"TimeseriesSurrogates.CycleShuffle","text":"CycleShuffle(n::Int = 7, Ïƒ = 0.5) <: Surrogate\n\nCycle shuffled surrogates[Theiler1995] that identify successive local peaks in the data and shuffle the cycles in-between the peaks. Similar to BlockShuffle, but here the \"blocks\" are defined as follows:\n\nThe timeseries is smoothened via convolution with a Gaussian (DSP.gaussian(n, Ïƒ)).\nLocal maxima of the smoothened signal define the peaks, and thus the blocks in between them.\nThe first and last index of timeseries can never be peaks and thus signals that should have peaks very close to start or end of the timeseries may not perform well. In addition, points before the first or after the last peak are never shuffled.\nThe defined blocks are randomly shuffled as in BlockShuffle.\n\nCSS are used to test the null hypothesis that the signal is generated by a periodic oscillator with no dynamical correlation between cycles, i.e. the evolution of cycles is not deterministic.\n\nSee also PseudoPeriodic.\n\n[Theiler1995]: J. Theiler, On the evidence for low-dimensional chaos in an epileptic electroencephalogram, Phys. Lett. A 196\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.CircShift","page":"Surrogate data","title":"TimeseriesSurrogates.CircShift","text":"CircShift(n) <: Surrogate\n\nSurrogates that are circularly shifted versions of the original timeseries.\n\nn can be an integer (meaning to shift for n indices), or any vector of integers, which which means that each surrogate is shifted by an integer, selected randomly among the entries in n.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.RandomFourier","page":"Surrogate data","title":"TimeseriesSurrogates.RandomFourier","text":"RandomFourier(phases = true) <: Surrogate\n\nA surrogate that randomizes the Fourier components of the signal in some manner. If phases==true, the phases are randomized, otherwise the amplitudes are randomized. FT is an alias for RandomFourier.\n\nRandom Fourier phase surrogates[Theiler1991] preserve the autocorrelation function, or power spectrum, of the original signal. Random Fourier amplitude surrogates preserve the mean and autocorrelation function but do not preserve the variance of the original. Random amplitude surrogates are not common in the literature, but are provided for convenience.\n\nRandom phase surrogates can be used to test the null hypothesis that the original signal was produced by a linear Gaussian process [Theiler1991].\n\n[Theiler1991]: J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1â€“4) (1992) 77â€“94.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.TFTS","page":"Surrogate data","title":"TimeseriesSurrogates.TFTS","text":"TFTS(fÏµ::Real)\n\nA truncated Fourier transform surrogate[Nakamura2006] (TFTS).\n\nTFTS surrogates are generated by leaving some frequencies untouched when performing the phase shuffling step (as opposed to randomizing all frequencies, like for RandomFourier surrogates).\n\nThese surrogates were designed to deal with data with irregular fluctuations superimposed over long term trends (by preserving low frequencies)[Nakamura2006]. Hence, TFTS surrogates can be used to test the null hypothesis that the signal is a stationary linear system generated the irregular fluctuations part of the signal[Nakamura2006].\n\nControlling the truncation of the spectrum\n\nThe truncation parameter fÏµ âˆˆ [-1, 0) âˆª (0, 1] controls which parts of the spectrum are preserved.\n\nIf fÏµ > 0, then fÏµ indicates the ratio of high frequency domain to the entire frequency domain.   For example, fÏµ = 0.5 preserves 50% of the frequency domain (randomizing the higher   frequencies, leaving low frequencies intact).\nIf fÏµ < 0, then fÏµ indicates ratio of low frequency domain to the entire frequency domain.   For example, fÏµ = -0.2 preserves 20% of the frequency domain (leaving higher frequencies intact,   randomizing the lower frequencies).\nIf fÏµ Â± 1, then all frequencies are randomized. The method is then equivalent to   RandomFourier.\n\nThe appropriate value of fÏµ strongly depends on the data and time series length, and must be manually determined[Nakamura2006], for example by comparing periodograms for the time series and the surrogates.\n\n[Nakamura2006]: Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.AAFT","page":"Surrogate data","title":"TimeseriesSurrogates.AAFT","text":"AAFT()\n\nAn amplitude-adjusted-fourier-transform surrogate[Theiler1991].\n\nAAFT have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data.\n\nAAFT can be used to test the null hypothesis that the data come from a monotonic nonlinear transformation of a linear Gaussian process (also called integrated white noise)[Theiler1991].\n\n[Theiler1991]: J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1â€“4) (1992) 77â€“94.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.TAAFT","page":"Surrogate data","title":"TimeseriesSurrogates.TAAFT","text":"TAAFT(fÏµ)\n\nAn truncated version of the amplitude-adjusted-fourier-transform surrogate[Theiler1991][Nakamura2006].\n\nThe truncation parameter and phase randomization procedure is identical to TFTS, but here an additional step of rescaling back to the original data is performed. This preserves the amplitude distribution of the original data.\n\n[Theiler1991]: J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1â€“4) (1992) 77â€“94.\n\n[Nakamura2006]: Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.IAAFT","page":"Surrogate data","title":"TimeseriesSurrogates.IAAFT","text":"IAAFT(M = 100, tol = 1e-6, W = 75)\n\nAn iteratively adjusted amplitude-adjusted-fourier-transform surrogate[SchreiberSchmitz1996].\n\nIAAFT surrogate have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data, but are improved relative to AAFT through iterative adjustment (which runs for a maximum of M steps). During the iterative adjustment, the periodograms of the original signal and the surrogate are coarse-grained and the powers are averaged over W equal-width frequency bins. The iteration procedure ends when the relative deviation between the periodograms is less than tol (or when M is reached).\n\nIAAFT, just as AAFT, can be used to test the null hypothesis that the data  come from a monotonic nonlinear transformation of a linear Gaussian process.\n\n[SchreiberSchmitz1996]: T. Schreiber; A. Schmitz (1996). \"Improved Surrogate Data for Nonlinearity Tests\". Phys. Rev. Lett. 77 (4)\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.AutoRegressive","page":"Surrogate data","title":"TimeseriesSurrogates.AutoRegressive","text":"AutoRegressive(n, method = LPCLevinson())\n\nAutoregressive surrogates of order-n. The autoregressive coefficients Ï† are estimated using DSP.lpc(x, n, method), and thus see the documentation of DSP.jl for possible methods.\n\nWhile these surrogates are obviously suited to test the null hypothesis whether the data are coming from a autoregressive process, the Fourier Transform-based surrogates are probably a better option. The current method is more like an explicit way to produce surrogates for the same hypothesis by fitting a model. It can be used as convient way to estimate autoregressive coefficients and automatically generate surrogates based on them.\n\nThe coefficients Ï† of the autoregressive fit can be found by doing\n\nsg = surrogenerator(x, AutoRegressive(n))\nÏ† = sg.init.Ï†\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.PseudoPeriodic","page":"Surrogate data","title":"TimeseriesSurrogates.PseudoPeriodic","text":"PseudoPeriodic(d, Ï„, Ï, shift=true) <: Surrogate\n\nCreate surrogates suitable for pseudo-periodic signals. They retain the periodic structure of the signal, while inter-cycle dynamics that are either deterministic or correlated noise are destroyed (for appropriate Ï choice). Therefore these surrogates are suitable to test the null hypothesis that the signal is a periodic orbit with uncorrelated noise[Small2001].\n\nArguments d, Ï„, Ï are as in the paper, the embedding dimension, delay time and noise radius. The method works by performing a delay coordinates ambedding via the library DynamicalSystems.jl. See its documentation for choosing appropriate values for d, Ï„. For Ï, we have implemented the method proposed in the paper in the function noiseradius.\n\nThe argument shift is not discussed in the paper. If shift=false we adjust the algorithm so that there is little phase shift between the periodic component of the original and surrogate data.\n\nSee also CycleShuffle.\n\n[Small2001]: Small et al., Surrogate test for pseudoperiodic time series data, Physical Review Letters, 87(18)\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.WLS","page":"Surrogate data","title":"TimeseriesSurrogates.WLS","text":"WLS(surromethod::Surrogate = IAAFT(), \n    rescale::Bool = true,\n    wt::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{16}())\n\nA wavelet surrogate generated by taking the maximal overlap discrete  wavelet transform (MODWT) of the signal, shuffling detail  coefficients at each dyadic scale using the provided surromethod, then taking the inverse transform to obtain a surrogate.\n\nCoefficient shuffling method\n\nIn contrast to the original  implementation where IAAFT is used, you may choose to use any surrogate  method from this package to perform the randomization of the detail  coefficients at each dyadic scale. Note: The iterative procedure after  the rank ordering step (step [v] in [Keylock2006]) is not performed in  this implementation.\n\nIf surromethod == IAAFT(), the wavelet surrogates preserves the local  mean and variance structure of the signal, but randomises nonlinear  properties of the signal (i.e. Hurst exponents)[Keylock2006]. These surrogates can therefore be used to test for changes in nonlinear properties of the  original signal.\n\nIn contrast to IAAFT surrogates, the IAAFT-wavelet surrogates also  preserves nonstationarity. Using other surromethods does not necessarily preserve nonstationarity.\n\nTo deal with nonstationary signals, Keylock (2006) recommends using a  wavelet with a high number of vanishing moments. Thus, the default is to use a Daubechies wavelet with 16 vanishing moments.\n\nRescaling\n\nIf rescale == true, then surrogate values are mapped onto the  values of the original time series, as in the AAFT algorithm. If rescale == false, surrogate values are not constrained to the  original time series values.\n\n[Keylock2006]: C.J. Keylock (2006). \"Constrained surrogate time series with preservation of the mean and variance structure\". Phys. Rev. E. 73: 036707. doi:10.1103/PhysRevE.73.036707.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#TimeseriesSurrogates.ShuffleDimensions","page":"Surrogate data","title":"TimeseriesSurrogates.ShuffleDimensions","text":"ShuffleDimensions()\n\nMultidimensional surrogates of input datasets (DelayEmbeddings.Dataset, which are also multidimensional) that have shuffled dimensions in each point.\n\nThese surrogates destroy the state space structure of the dataset and are thus suited to distinguish deterministic datasets from high dimensional noise.\n\n\n\n\n\n","category":"type"},{"location":"surrogate/#Utils","page":"Surrogate data","title":"Utils","text":"","category":"section"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"noiseradius","category":"page"},{"location":"surrogate/#TimeseriesSurrogates.noiseradius","page":"Surrogate data","title":"TimeseriesSurrogates.noiseradius","text":"noiseradius(x::AbstractVector, d::Int, Ï„, Ïs, n = 1) â†’ Ï\n\nUse the proposed* algorithm of[Small2001] to estimate optimal Ï value for PseudoPeriodic surrogates, where Ïs is a vector of possible Ï values.\n\n*The paper is ambiguous about exactly what to calculate. Here we count how many times we have pairs of length-2 that are identical in x and its surrogate, but are not also part of pairs of length-3.\n\nThis function directly returns the arg-maximum of the evaluated distribution of these counts versus Ï, use TimeseriesSurrogates._noiseradius with same arguments to get the actual distribution. n means to repeat Ï„he evaluation n times, which increases accuracy.\n\n[Small2001]: Small et al., Surrogate test for pseudoperiodic time series data, Physical Review Letters, 87(18)\n\n\n\n\n\n","category":"function"},{"location":"s_measure/#Smeasure_overview","page":"S-measure","title":"S-measure","text":"","category":"section"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"s_measure","category":"page"},{"location":"s_measure/#CausalityTools.SMeasure.s_measure","page":"S-measure","title":"CausalityTools.SMeasure.s_measure","text":"s_measure(x, y, m::Int, Ï„, K::Int; distance_metric::Metric = SqEuclidean())\n\nS-measure test [1] for the directional dependence between data series.\n\nAlgorithm\n\nCreate an m-dimensional embeddings of both x and y, resulting in   N different m-dimensional embedding points  X = x_1 x_2 ldots x_N  and X = y_1 y_2 ldots y_N .  Ï„ controls the embedding lag.\nLet r_ij and s_ij be the indices of the K-th nearest neighbors   of x_i and y_i, respectively.\nCompute the the mean squared Euclidean distance to the K nearest neighbors   for each x_i, using the indices r_i j.\n\nR_i^(k)(x) = dfrac1k sum_i=1^k(x_i x_r_ij)^2\n\nCompute the y-conditioned mean squared Euclidean distance to the K nearest    neighbors for each x_i, now using the indices s_ij.\n\nR_i^(k)(xy) = dfrac1k sum_i=1^k(x_i x_s_ij)^2\n\nDefine the following measure of independence, where 0 leq S leq 1, and    low values indicate independence and values close to one occur for    synchronized signals.\n\nS^(k)(xy) = dfrac1N sum_i=1^N dfracR_i^(k)(x)R_i^(k)(xy)\n\nReferences\n\nQuian Quiroga, R., Arnhold, J. & Grassberger, P. [2000] â€œLearning driver-response relationships from synchronization patterns,â€  Phys. Rev. E61(5), 5142â€“5148.\n\n\n\n\n\n","category":"function"},{"location":"joint_distance_distribution/#joint_distance_distribution_overview","page":"Joint distance distribution","title":"Joint distance distribution","text":"","category":"section"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jdd(::Any, ::Any)","category":"page"},{"location":"joint_distance_distribution/#CausalityTools.JointDistanceDistribution.jdd-Tuple{Any,Any}","page":"Joint distance distribution","title":"CausalityTools.JointDistanceDistribution.jdd","text":"jdd(source, target; distance_metric = SqEuclidean(), \n    B::Int = 10, D::Int = 2, Ï„::Int = 1) â†’ Vector{Float64}\n\nCompute the joint distance distribution [1] from source to target using  the provided distance_metric, with B controlling the number of subintervals,  D the embedding dimension and Ï„ the embedding lag.\n\nExample\n\nusing CausalityTools\nx, y = rand(1000), rand(1000)\n\njdd(x, y)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.    Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances. \nD: Embedding dimension.\nÏ„: Embedding delay.\n\nReferences\n\n[1] AmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"joint_distance_distribution/#Hypothesis-test-on-the-joint-distance-distribution","page":"Joint distance distribution","title":"Hypothesis test on the joint distance distribution","text":"","category":"section"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"For the joint distance distribution to indicate a causal influence, it must be significantly  skewed towards positive values.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Providing the OneSampleTTest type as the first  argument yields a one sample t-test on the joint distance distribution. From this test, you can extract p-values and obtain  confidence intervals like in HypothesisTests.jl as usual.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jdd(::Type{OneSampleTTest}, ::Any, ::Any)","category":"page"},{"location":"joint_distance_distribution/#CausalityTools.JointDistanceDistribution.jdd-Tuple{Type{OneSampleTTest},Any,Any}","page":"Joint distance distribution","title":"CausalityTools.JointDistanceDistribution.jdd","text":"jdd(test::OneSampleTTest, source, target;\n    distance_metric = SqEuclidean(), B::Int = 10, D::Int = 2, Ï„::Int = 1, \n    Î¼0 = 0.0) â†’ OneSampleTTest\n\nPerform a one sample t-test to check that the joint distance distribution [1]  computed from source to target is biased towards positive values, using the null  hypothesis that the mean of the distribution is Î¼0.\n\nThe interpretation of the t-test is that if we can reject the null, then the  joint distance distribution is biased towards positive values, and then there  exists an underlying coupling from source to target. \n\nExample\n\nusing CausalityTools, HypothesisTests\nx, y = rand(1000), rand(1000)\n\njdd(OneSampleTTest, x, y)\n\nwhich gives \n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         0.0\n    point estimate:          0.06361857324022721\n    95% confidence interval: (0.0185, 0.1087)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0082\n\nDetails:\n    number of observations:   20\n    t-statistic:              2.9517208721082873\n    degrees of freedom:       19\n    empirical standard error: 0.0215530451545668\n\nThe lower bound of the confidence interval for the mean of the joint  distance distribution is 0.0185 at confidence level Î± = 0.05. The  meaning that the test falsely detected causality from x to y between these two random time series. To get the confidence intervals at confidence level Î±, use confinf(jdd, Î±). If you just want the  p-value at 95% confidence, use pvalue(jdd, tail = :left)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.    Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances. \nD: Embedding dimension.\nÏ„: Embedding delay.\nÎ¼0: The hypothetical mean value of the joint distance distribution if there    is no coupling between x and y (default is Î¼0 = 0.0).\n\nReferences\n\n[1] AmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"#Introduction","page":"Overview","title":"Introduction","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"CausalityTools is a Julia package providing algorithms for detecting dynamical relations  between variables of complex systems based on time series data.","category":"page"},{"location":"cross_mapping/#Cross-mapping","page":"Cross mapping","title":"Cross mapping","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Given two data series, the putative driver and the putative response, the the crossmap(driver, response; kwargs...) function computes how well a delay embedding of response predicts scalar values of driver. This is the original cross mapping algorithm from Sugihara et al. [1].","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"To perform lagged CCM analysis [2] as Ye et al., you can tune the Î½ parameter.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"CrossMappings.crossmap","category":"page"},{"location":"cross_mapping/#CrossMappings.crossmap","page":"Cross mapping","title":"CrossMappings.crossmap","text":"crossmap(source, target;\n    dim::Int = 3,\n    Ï„::Int = 1,\n    libsize::Int = 10,\n    replace::Bool = false,\n    n_reps::Int = 100,\n    theiler_window::Int = 0,\n    tree_type = NearestNeighbors.KDTree,\n    distance_metric = Distances.Euclidean(),\n    correspondence_measure = StatsBase.cor,\n    Î·::Int = 0)\n\nAlgorithm\n\nCompute the cross mapping between a source series and a target series.\n\nArguments\n\nsource: The data series representing the putative source process.\ntarget: The data series representing the putative target process.\ndim: The dimension of the state space reconstruction (delay embedding)   constructed from the target series. Default is dim = 3.\nÏ„: The embedding lag for the delay embedding constructed from target.   Default is Ï„ = 1.\nÎ·: The prediction lag to use when predicting scalar values of source   fromthe delay embedding of target.   Î· > 0 are forward lags (causal; source's past influences target's future),   and Î· < 0 are backwards lags (non-causal; source's' future influences   target's past). Adjust the prediction lag if you   want to performed lagged ccm   (Ye et al., 2015).   Default is Î· = 0, as in   Sugihara et al. (2012).   Note: The sign of the lag Î· is organized to conform with the conventions in   TransferEntropy.jl, and is opposite to the convention used in the   rEDM package   (Ye et al., 2016).\nlibsize: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are n_reps)?\nn_reps: The number of times we draw a library of libsize points from the   delay embedding of target and try to predict source values. Equivalently,   how many times do we cross map for this value of libsize?   Default is n_reps = 100.\nreplace: Sample delay embedding points with replacement? Default is replace = true.\ntheiler_window: How many temporal neighbors of the delay embedding   point target_embedding(t) to exclude when searching for neighbors to   determine weights for predicting the scalar point source(t + Î·).   Default is theiler_window = 0.\ntree_type: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   BruteTree, KDTree or BallTree.\ndistance_metric: An instance of a Metric from Distances.jl. BallTree and BruteTree work with any Metric.   KDTree only works with the axis aligned metrics Euclidean, Chebyshev,   Minkowski and Cityblock. Default is metric = Euclidean() (note the instantiation of the metric).\ncorrespondence_measure: The function that computes the correspondence   between actual values of source and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is correspondence_measure = StatsBase.cor, which returns values on -1 1.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of 1 means perfect prediction.   Sugihara et al. (2012)   also proposes to use the root mean square deviation, for which a value of 0 would   be perfect prediction.\n\nReferences\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079. http://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750. https://www.nature.com/articles/srep14750\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016). https://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\n\n","category":"function"},{"location":"cross_mapping/#Cross-map-over-multiple-time-series-lengths","page":"Cross mapping","title":"Cross map over multiple time series lengths","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"To perform a convergent cross map analysis as in [1] one can apply the crossmap functions on time series of increasing length. The convergentcrossmap(driver, response, timeserieslengths; kwargs...) function does so by applying crossmap for each time series length in timeserieslengths, where time windows always start at the first data point.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"CrossMappings.convergentcrossmap","category":"page"},{"location":"cross_mapping/#CrossMappings.convergentcrossmap","page":"Cross mapping","title":"CrossMappings.convergentcrossmap","text":"convergentcrossmap(source,\n        target,\n        timeseries_lengths;\n        summarise::Bool = true,\n        average_measure::Symbol = :median,\n        uncertainty_measure::Symbol = :quantile,\n        quantiles = [0.327, 0.673],\n        kwargs...)\n\nAlgorithm\n\nCompute the cross mapping between a source series and a target series over different timeseries_lengths. If summarise = true, then call ccm_with_summary. If summarise = false, then call ccm (returns raw crossmap skills).\n\nArguments\n\nsource: The data series representing the putative source process.\ntarget: The data series representing the putative target process.\ntimeseries_lengths: Time series length(s) for which to compute the   cross mapping(s).\n\nSummary keyword arguments\n\nsummarise: Should cross map skills be summarised for each time series length?   Default is summarise = true.\naverage_measure: Either :median or :mean. Default is :median.\nuncertainty_measure: Either :quantile or :std. Default is :quantile.\nquantiles: Compute uncertainty over quantile(s) if uncertainty_measure   is :quantile. Default is [0.327, 0.673], roughly corresponding to 1s for   normally distributed data.\n\nKeyword arguments to crossmap\n\ndim: The dimension of the state space reconstruction (delay embedding)   constructed from the target series. Default is dim = 3.\nÏ„: The embedding lag for the delay embedding constructed from target.   Default is Ï„ = 1.\nÎ·: The prediction lag to use when predicting scalar values of source   fromthe delay embedding of target.   Î· > 0 are forward lags (causal; source's past influences target's future),   and Î· < 0 are backwards lags (non-causal; source's' future influences   target's past). Adjust the prediction lag if you   want to performed lagged ccm   (Ye et al., 2015).   Default is Î· = 0, as in   Sugihara et al. (2012).   Note: The sign of the lag Î· is organized to conform with the conventions in   TransferEntropy.jl, and is opposite to the convention used in the   rEDM package   (Ye et al., 2016).\nlibsize: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are n_reps)?\nn_reps: The number of times we draw a library of libsize points from the   delay embedding of target and try to predict source values. Equivalently,   how many times do we cross map for this value of libsize?   Default is n_reps = 100.\nreplace: Sample delay embedding points with replacement? Default is replace = true.\ntheiler_window: How many temporal neighbors of the delay embedding   point target_embedding(t) to exclude when searching for neighbors to   determine weights for predicting the scalar point source(t + Î·).   Default is theiler_window = 0.\ntree_type: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   BruteTree, KDTree or BallTree.\ndistance_metric: An instance of a Metric from Distances.jl. BallTree and BruteTree work with any Metric.   KDTree only works with the axis aligned metrics Euclidean, Chebyshev,   Minkowski and Cityblock. Default is metric = Euclidean() (note the instantiation of the metric).\ncorrespondence_measure: The function that computes the correspondence   between actual values of source and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is correspondence_measure = StatsBase.cor, which returns values on -1 1.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of 1 means perfect prediction.   Sugihara et al. (2012)   also proposes to use the root mean square deviation, for which a value of 0 would   be perfect prediction.\n\nReferences\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079. http://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750. https://www.nature.com/articles/srep14750\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016). https://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\n\n","category":"function"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"[1]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. Science. https://doi.org/10.1126/science.1227079","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"[2]: Ye, H., Deyle, E. R., Gilarranz, L. J., & Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific Reports. https://doi.org/10.1038/srep14750","category":"page"}]
}
